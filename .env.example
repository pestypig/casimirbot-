ENABLE_ESSENCE=1      # set to 0 to disable Essence APIs
ENABLE_AGI=1          # set to 0 to disable AGI planner routes
ENABLE_AGI_UI=1
ENABLE_PERSONA_UI=1
ENABLE_MEMORY_UI=1
ENABLE_PHYSICS=0
ENABLE_LATTICE_WATCHER=0
LATTICE_WATCHER_DEBOUNCE_MS=350
VITE_CHAT_CONTEXT_BUDGET_TOKENS=8192
VITE_HELIX_DEV_MOCKS=1     # set to 0 to surface real Helix API errors instead of mocks
VITE_DEBUG_VIZ_HUD=1       # set to 1 to show Viz Diagnostics HUD on Helix Start
VITE_ENABLE_LATTICE_WATCHER=0
ENABLE_RESONANCE_ORCHESTRA=1
VITE_ENABLE_RESONANCE_ORCHESTRA=1

# LLM runtime selection and paths (user will set real paths locally)
LLM_RUNTIME=llama.cpp
LLM_MODEL_PATH=/models/8b-q4.gguf
LLM_DRAFT_MODEL_PATH=/models/3b-q4.gguf
LLM_USE_SPECULATIVE=true

# KV budget (~1.5 GB by default for laptop VRAM)
KV_BUDGET_BYTES=1500000000
KV_EVICT_STRATEGY=oldest

# RAG / Memory
EMBEDDING_SPACE=text-bert-base
MEM_TOPK=6

# Diffusion defaults (adapter will exist later)
DIFF_ENGINE=sd15-lcm
DIFF_STEPS=4
DIFF_SLICING=true

# Persistence / storage
USE_INMEM_MEMORY=1
ENABLE_PGVECTOR=0
STORAGE_BACKEND=fs
DATA_DIR=./data/essence
REDIS_URL=
MEDIA_QUEUE_CONCURRENCY=1
MEDIA_GPU_CONCURRENCY=1
MEDIA_QUEUE_BYPASS=0
S3_BUCKET=essence-artifacts
S3_REGION=us-east-1
S3_ENDPOINT=
S3_FORCE_PATH_STYLE=1
S3_ACCESS_KEY_ID=
S3_SECRET_ACCESS_KEY=

# Auth
ENABLE_AUTH=0
ALLOW_ADMIN=0
JWT_SECRET=super-secret

# GPU safety
GPU_TEMP_MAX_C=82
GPU_TEMP_OVERRIDE=

# === Knowledge Projects / Attachments ===
ENABLE_KNOWLEDGE_PROJECTS=1
MAX_KNOWLEDGE_CONTEXT_BYTES=262144         # 256 KB total payload per plan
MAX_KNOWLEDGE_FILES_PER_PROJECT=64
KNOWLEDGE_ALLOWED_MIME=text/plain,text/markdown,application/json,audio/wav,audio/mpeg
# Planner hinting toggles (optional)
ENABLE_KNOWLEDGE_PROMPT_HINTS=1

# === Hull Mode (offline) ===
HULL_MODE=0
# Add external hosts you trust in Hull Mode (comma-separated). Example to allow OpenAI:
# HULL_ALLOW_HOSTS=api.openai.com,127.0.0.1,::1,localhost,*.hull
HULL_ALLOW_HOSTS=127.0.0.1,::1,localhost,*.hull
LLM_POLICY=local
LLM_LOCAL_BASE=http://127.0.0.1:11434
LLM_LOCAL_MODEL=local-7b-instruct
ENABLE_CAPSULE_IMPORT=1
HULL_CA_PUBKEY_PATH=./ops/hull-ca.pub

# === LLM (OpenAI-compatible HTTP, e.g., llama.cpp server, vLLM, Ollama / OpenAI) ===
LLM_HTTP_BASE=
LLM_HTTP_API_KEY=
LLM_HTTP_MODEL=gpt-4o-mini
LLM_HTTP_TEMPERATURE=0.2

# === Vision (OpenAI-compatible Chat Completions with images) ===
VISION_HTTP_BASE=
VISION_HTTP_API_KEY=
VISION_HTTP_MODEL=gpt-4o-mini
VISION_HTTP_RPM=60

# === STT (OpenAI /audio/transcriptions or generic) ===
WHISPER_HTTP_MODE=openai     # openai | generic
WHISPER_HTTP_URL=
WHISPER_HTTP_API_KEY=

# === Diffusion (AUTOMATIC1111 SD WebUI or ComfyUI) ===
DIFF_HTTP_ENGINE=sdwebui     # sdwebui | comfyui
DIFF_HTTP_URL=
DIFF_STEPS=6
DIFF_WIDTH=512
DIFF_HEIGHT=512
DIFF_SAMPLER=LCM
DIFF_SEED=-1                 # -1 = backend random

# === Feature gates ===
ENABLE_REFLECTION=1
ENABLE_LOG_TEXT=1
ENABLE_EVAL_UI=0
ENABLE_TRACE_EXPORT=0
ENABLE_POLICY_REASONS=1
ENABLE_EVAL_REPLAY=0
ENABLE_ESSENCE_PROPOSALS=1
ENABLE_PROPOSAL_JOB_RUNNER=1

# === Eval ===
EVAL_TASKS_FILE=./tests/evals/tasks.smoke.json
EVAL_SUCCESS_TARGET=0.70
EVAL_BASE_URL=http://localhost:3000

# === Specialists ===
ENABLE_SPECIALISTS=1
SPECIALISTS_MAX_REPAIR=1
SPECIALISTS_MAX_LATENCY_MS=120000
SPECIALISTS_DEFAULT_PERSONA=persona:demo
SOLVER_ALLOWLIST=
VERIFIER_ALLOWLIST=

ENABLE_PY_CHECKERS=0
PYTHON_BIN=python3
ENABLE_TRACE_API=1
TRACE_SSE_BUFFER=200
TRACE_TTL_MS=21600000

# Debate mode
ENABLE_DEBATE=1
DEBATE_MAX_ROUNDS=6
DEBATE_MAX_WALL_MS=900000
DEBATE_VERIFIERS=math.sympy.verify,code.verify.tests
ENABLE_DEBATE_SEARCH=1
DEBATE_SEARCH_DEFAULT=1    # if 1 and UI gate is on, select the Debate chip by default

# Debate UI (client)
VITE_ENABLE_DEBATE_UI=1
VITE_DEBATE_SSE_URL=/api/agi/tools/logs/stream
VITE_TRACE_API_BASE=/api/agi/trace
VITE_ENABLE_DEBATE_SEARCH=1
VITE_DEBATE_SEARCH_DEFAULT=1

# Local binary (no HTTP) LLM adapter
ENABLE_LLM_LOCAL_SPAWN=1
LLM_LOCAL_CMD=./bin/llama-cli              # your local binary path
LLM_LOCAL_ARGS_BASE=--n-gpu-layers=20 --ctx-size=4096
LLM_LOCAL_MODEL=./models/Local-7B-Q4.gguf  # file path
LLM_LOCAL_MAX_TOKENS=512
LLM_LOCAL_TEMP=0.2
LLM_LOCAL_SEED=42


# --- OpenAI wiring profile (example) ---
# Core posture
# ENABLE_ESSENCE=1
# ENABLE_AGI=1
# ENABLE_LOG_TEXT=1
# HULL_MODE=1
# HULL_ALLOW_HOSTS=api.openai.com

# Text LLM (chat/completions)
# LLM_HTTP_BASE=https://api.openai.com
# LLM_HTTP_API_KEY=<YOUR_OPENAI_KEY>
# LLM_HTTP_MODEL=gpt-4o-mini

# Vision (image â†’ text) also uses chat/completions
# VISION_HTTP_BASE=https://api.openai.com
# VISION_HTTP_API_KEY=<YOUR_OPENAI_KEY>
# VISION_HTTP_MODEL=gpt-4o-mini

# Speech-to-Text (Whisper)
# WHISPER_HTTP_URL=https://api.openai.com
# WHISPER_HTTP_API_KEY=<YOUR_OPENAI_KEY>
# WHISPER_HTTP_MODE=openai

# Optional console niceties
# ENABLE_TRACE_EXPORT=1
# ENABLE_POLICY_REASONS=1
# ENABLE_EVAL_REPLAY=1


