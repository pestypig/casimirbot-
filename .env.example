# PORT=
# API_PROXY_TARGET is only needed when the Vite dev server proxies to a separate API.
# With Express + Vite on the same port, leave this unset to avoid proxy loops.
# API_PROXY_TARGET=http://localhost:3000

ENABLE_ESSENCE=1      # set to 0 to disable Essence APIs
ENABLE_AGI=1          # set to 0 to disable AGI planner routes
ENABLE_AGI_UI=1
ENABLE_PERSONA_UI=1
ENABLE_MEMORY_UI=1
ENABLE_PHYSICS=1
HELIX_MODEL_MODE=raw  # raw = Mk.1/paper profile (requested gap); calibrated = enforce mechanical guard
OPENAI_API_KEY=
# LLM_HTTP_API_KEY=
HELIX_SURFACE_API_KEY=
PHYSICS_MODEL=gpt-4o-mini
BACKEND_PHYSICS_ASK=openai
PHYSICS_ASK_OPENAI_MODEL=gpt-4.1-mini
PHYSICS_ASK_LOCAL_MODEL=llama3.1-70b-instruct
VITE_CHAT_CONTEXT_BUDGET_TOKENS=8192
VITE_HELIX_DEV_MOCKS=0     # set to 0 to surface real Helix API errors instead of mocks
VITE_DEBUG_VIZ_HUD=1       # set to 1 to show Viz Diagnostics HUD on Helix Start
# Helix Ask (desktop bar)
VITE_HELIX_ASK_MODE=grounded     # grounded | execute | local
VITE_HELIX_ASK_USE_KNOWLEDGE=1
VITE_HELIX_ASK_MAX_TOKENS=4096
VITE_HELIX_ASK_OUTPUT_TOKENS=1600
VITE_HELIX_ASK_CONTEXT_TOKENS=3072
VITE_HELIX_ASK_CONTEXT_FILES=48
VITE_HELIX_ASK_CONTEXT_CHARS=2400
VITE_HELIX_ASK_SEARCH_FALLBACK=1
HELIX_ASK_TWO_PASS=0
HELIX_ASK_SCAFFOLD_TOKENS=1024
HELIX_ASK_EVIDENCE_TOKENS=512
HELIX_ASK_DEFAULT_VERBOSITY=extended
HELIX_ASK_ANSWER_MAX_TOKENS=2048
HELIX_ASK_SHORT_ANSWER_RETRY_MAX=1
HELIX_ASK_SHORT_ANSWER_MIN_SENTENCES=0
HELIX_ASK_SHORT_ANSWER_MIN_TOKENS=0
HELIX_ASK_FAILURE_MAX=1          # open circuit after N consecutive failures (0 disables)
HELIX_ASK_FAILURE_COOLDOWN_MS=120000
HELIX_ASK_ARBITER_REPO_RATIO=0.45
HELIX_ASK_ARBITER_HYBRID_RATIO=0.25
HELIX_ASK_EVIDENCE_CRITIC=0
HELIX_ASK_EVIDENCE_CLAIM_GATE=1
HELIX_ASK_EVIDENCE_CLAIM_MAX=6
HELIX_ASK_EVIDENCE_CLAIM_MIN_RATIO=0.2
HELIX_ASK_EVIDENCE_CLAIM_MIN_TOKENS=1
HELIX_ASK_EVIDENCE_CLAIM_SUPPORT_RATIO=0.5
HELIX_ASK_AMBIGUITY_GATE=1
HELIX_ASK_AMBIGUITY_RESOLVER=1
HELIX_ASK_AMBIGUITY_SHORT_TOKENS=4
HELIX_ASK_AMBIGUITY_MIN_SCORE=8
HELIX_ASK_AMBIGUITY_MARGIN_MIN=4
HELIX_ASK_AMBIGUITY_CLUSTER_TOPK=18
HELIX_ASK_AMBIGUITY_CLUSTER_MARGIN_MIN=0.35
HELIX_ASK_AMBIGUITY_CLUSTER_ENTROPY_MAX=0.6
HELIX_ASK_AMBIGUITY_LABEL_LLM=0
HELIX_ASK_AMBIGUOUS_TERM_MIN_LEN=5
HELIX_ASK_AMBIGUOUS_MAX_TERMS=2
HELIX_ASK_LONGPROMPT_CHUNK_TOKENS=420
HELIX_ASK_LONGPROMPT_CHUNK_OVERLAP=80
HELIX_ASK_LONGPROMPT_TOPK_CANDIDATES=18
HELIX_ASK_LONGPROMPT_TOPM_SELECTED=8
HELIX_ASK_LONGPROMPT_CARD_TOKENS=256
HELIX_ASK_LONGPROMPT_MAX_CARDS=12
HELIX_ASK_LONGPROMPT_TRIGGER_TOKENS=640
HELIX_ASK_LONGPROMPT_OVERHEAD_TOKENS=360
HELIX_ASK_REPORT_MODE=1
HELIX_ASK_REPORT_TRIGGER_TOKENS=220
HELIX_ASK_REPORT_TRIGGER_CHARS=1400
HELIX_ASK_REPORT_TRIGGER_BLOCKS=4
HELIX_ASK_REPORT_MAX_BLOCKS=12
HELIX_ASK_REPORT_BLOCK_CHAR_LIMIT=900
HELIX_ASK_DRIFT_REPAIR=1
HELIX_ASK_DRIFT_REPAIR_MAX=1
HELIX_ASK_OVERFLOW_RETRY=1
HELIX_ASK_OVERFLOW_RETRY_POLICY=drop_context_then_drop_output_then_retry
HELIX_ASK_BELIEF_SUPPORT_MIN_RATIO=0.25
HELIX_ASK_BELIEF_SUPPORT_MIN_TOKENS=2
HELIX_ASK_RATTLING_PERTURB=1
HELIX_ASK_VARIANT_SELECTION=0
HELIX_ASK_VARIANT_MIN_SENTENCES=2
HELIX_ASK_VARIANT_MIN_RATIO=0.5
HELIX_ASK_TRAINING_TRACE=1
HELIX_ASK_AGENT_LOOP=1
HELIX_ASK_AGENT_LOOP_MAX_STEPS=3
HELIX_ASK_AGENT_ACTION_BUDGET_MS=20000
HELIX_ASK_AGENT_LOOP_BUDGET_MS=120000
HELIX_ASK_AGENT_CODE_FIRST=1
HELIX_ASK_QUERY_HINTS_BLOCKS=1
HELIX_ASK_QUERY_TOKENS_BLOCK=96
HELIX_ASK_SESSION_MEMORY=1
HELIX_ASK_SESSION_TTL_MS=1800000
HELIX_ASK_SESSION_MAX_SLOTS=12
HELIX_ASK_SESSION_MAX_FILES=12
HELIX_ASK_SESSION_MAX_ALIASES=6
HELIX_ASK_SESSION_MAX_CLARIFY=4
HELIX_ASK_SESSION_MAX_OPEN_SLOTS=8
HELIX_ASK_SESSION_MAX_ATTEMPTS=24
HELIX_ASK_SESSION_MAX_TOPICS=8
HELIX_ASK_SESSION_PERSIST_PATH=artifacts/helix-ask-session-memory.json
# Runtime resilience (server)
FATAL_EXIT_ON_ERROR=1           # exit on uncaught errors in production to allow supervisor restart
FATAL_EXIT_DELAY_MS=1500
TRUST_PROXY=0                   # set to 1 or hop count to honor X-Forwarded-For
RATE_LIMIT_ENABLED=1
RATE_LIMIT_API_WINDOW_MS=60000
RATE_LIMIT_API_MAX=240
RATE_LIMIT_ASK_JOBS_WINDOW_MS=60000
RATE_LIMIT_ASK_JOBS_MAX=1200
HELIX_COMMAND_RATE_LIMIT_PER_MINUTE=30
HELIX_COMMAND_RATE_LIMIT_WINDOW_MS=60000
HELIX_ASK_CONCURRENCY_MAX=4
HTTP_SERVER_REQUEST_TIMEOUT_MS=120000
HTTP_SERVER_HEADERS_TIMEOUT_MS=65000
HTTP_SERVER_KEEPALIVE_TIMEOUT_MS=5000
RUNTIME_ARTIFACT_RETRY_MS=30000
RUNTIME_ARTIFACT_BREAKER_FAIL_MAX=3
RUNTIME_ARTIFACT_BREAKER_COOLDOWN_MS=60000
LLM_SPAWN_BREAKER_FAIL_MAX=3
LLM_SPAWN_BREAKER_COOLDOWN_MS=60000
SENTRY_DSN=
SENTRY_ENV=
SENTRY_RELEASE=
SENTRY_TRACES_SAMPLE_RATE=0
HELIX_ASK_RETRIEVAL_RETRY_ENABLED=1
HELIX_ASK_RETRIEVAL_RETRY_TOPK_BONUS=4
HELIX_ASK_FORMAT_ENFORCEMENT_LEVEL=strict
HELIX_ASK_SOFT_EXPANSION_MAX_SENTENCES=0
HELIX_ASK_SWEEP_OVERRIDES=0
HELIX_ASK_REGRESSION_TIMEOUT_MS=300000
ENABLE_RESONANCE_ORCHESTRA=1
VITE_ENABLE_RESONANCE_ORCHESTRA=1

HYBRID_COLLAPSE_MODE=deterministic_hash_v1
VITE_HYBRID_COLLAPSE_MODE=deterministic_hash_v1
FORCE_FULL_PLAN=1
VITE_FORCE_FULL_PLAN=1

# Local call-spec (edge micro-LLM) toggle + endpoint
ENABLE_LOCAL_CALL_SPEC=0
VITE_ENABLE_LOCAL_CALL_SPEC=0
VITE_LOCAL_CALL_SPEC_URL=http://localhost:11434/api/local-call-spec

# Optional local chooser for collapse
VITE_LOCAL_CHOOSER_URL=

# Optional DP collapse side-effect bounds (kg-normalized heating, diffusion, force-noise)
DP_HEATING_W_KG_MAX=
DP_MOMENTUM_DIFFUSION_MAX=
DP_FORCE_NOISE_MAX=

# Local call-spec (server proxy to local Ollama)
LOCAL_CALL_SPEC_URL=http://127.0.0.1:11434/api/local-call-spec

# Local TTS / STT (Piper / Whisper via local HTTP)
ENABLE_LOCAL_TTS=1
VITE_ENABLE_LOCAL_TTS=0
LOCAL_TTS_URL=http://127.0.0.1:8000/api/tts
VITE_LOCAL_TTS_URL=http://127.0.0.1:8000/api/tts

ENABLE_LOCAL_STT=0
VITE_ENABLE_LOCAL_STT=0
LOCAL_STT_URL=http://127.0.0.1:11434/api/stt
VITE_LOCAL_STT_URL=http://127.0.0.1:11434/api/stt

# LLM runtime selection and paths (user will set real paths locally)
LLM_RUNTIME=llama.cpp
LLM_MODEL_PATH=/models/8b-q4.gguf
LLM_DRAFT_MODEL_PATH=/models/3b-q4.gguf
LLM_USE_SPECULATIVE=true
LLM_LOCAL_CONCURRENCY=1
LLM_LOCAL_SPAWN_TIMEOUT_MS=240000
LLM_LOCAL_CONCURRENCY=1

# KV budget (~1.5 GB by default for laptop VRAM)
KV_BUDGET_BYTES=1500000000
KV_EVICT_STRATEGY=oldest

# RAG / Memory
EMBEDDING_SPACE=text-bert-base
MEM_TOPK=6

# Diffusion defaults (adapter will exist later)
DIFF_ENGINE=sd15-lcm
# DIFF_STEPS=
DIFF_SLICING=true

# Persistence / storage
USE_INMEM_MEMORY=1
ENABLE_PGVECTOR=0
STORAGE_BACKEND=fs
DATA_DIR=./data/essence
AGI_ARTIFACTS_DIR=.cache/artifacts
REDIS_URL=
MEDIA_QUEUE_CONCURRENCY=1
MEDIA_GPU_CONCURRENCY=1
MEDIA_QUEUE_BYPASS=0
S3_BUCKET=essence-artifacts
S3_REGION=us-east-1
S3_ENDPOINT=
S3_FORCE_PATH_STYLE=1
S3_ACCESS_KEY_ID=
S3_SECRET_ACCESS_KEY=

# Auth
ENABLE_AUTH=0
ENABLE_AGI_AUTH=0     # set to 1 to require JWT on /api/agi/* even if global auth off
ALLOW_ADMIN=0
JWT_SECRET=
AGI_TENANT_REQUIRED=  # unset uses ENABLE_AUTH/ENABLE_AGI_AUTH; set to 1 to always require tenant id
AGI_TENANT_HEADERS=x-tenant-id,x-customer-id,x-org-id

# GPU safety
GPU_TEMP_MAX_C=82
GPU_TEMP_OVERRIDE=

# === Knowledge Projects / Attachments ===
ENABLE_KNOWLEDGE_PROJECTS=1
MAX_KNOWLEDGE_CONTEXT_BYTES=262144         # 256 KB total payload per plan
MAX_KNOWLEDGE_FILES_PER_PROJECT=64
KNOWLEDGE_ALLOWED_MIME=text/plain,text/markdown,application/json,application/pdf,audio/wav,audio/mpeg
# Planner hinting toggles (optional)
ENABLE_KNOWLEDGE_PROMPT_HINTS=1

# === Hull Mode (offline) ===
HULL_MODE=0
# Add external hosts you trust in Hull Mode (comma-separated). Example to allow OpenAI:
# HULL_ALLOW_HOSTS=api.openai.com,127.0.0.1,::1,localhost,*.hull
HULL_ALLOW_HOSTS=127.0.0.1,::1,localhost,*.hull
LLM_POLICY=http
LLM_LOCAL_BASE=http://127.0.0.1:11434
# LLM_LOCAL_MODEL=
ENABLE_CAPSULE_IMPORT=1
HULL_CA_PUBKEY_PATH=./ops/hull-ca.pub

# === LLM (OpenAI-compatible HTTP, e.g., llama.cpp server, vLLM, Ollama / OpenAI) ===
LLM_HTTP_BASE=https://api.openai.com
LLM_HTTP_API_KEY=
LLM_HTTP_MODEL=gpt-4o-mini
LLM_HTTP_TEMPERATURE=0.2

# === Luma summarizer (OpenAI/ollama/vLLM) ===
LUMA_PROVIDER=ollama
LUMA_MODEL=mistral:7b-instruct
LUMA_BASE_URL=http://127.0.0.1:5173
OLLAMA_ENDPOINT=http://127.0.0.1:11434
LORA_ADAPTER=
OLLAMA_PROFILE_MODEL=mistral:7b-instruct

# === Vision (OpenAI-compatible Chat Completions with images) ===
VISION_HTTP_BASE=
VISION_HTTP_API_KEY=
VISION_HTTP_MODEL=gpt-4o-mini
VISION_HTTP_RPM=60

# === STT (OpenAI /audio/transcriptions or generic) ===
WHISPER_HTTP_MODE=openai     # openai | generic
WHISPER_HTTP_URL=
WHISPER_HTTP_API_KEY=

# === Diffusion (AUTOMATIC1111 SD WebUI or ComfyUI) ===
DIFF_HTTP_ENGINE=sdwebui     # sdwebui | comfyui
DIFF_HTTP_URL=
DIFF_STEPS=6
DIFF_WIDTH=512
DIFF_HEIGHT=512
DIFF_SAMPLER=LCM
DIFF_SEED=-1                 # -1 = backend random

# === Solar coherence (Star Watcher) ===
SOLAR_COCO_MAX_PIXELS=4000000          # per-frame target pixels (downscale to this budget)
SOLAR_COCO_INPUT_PIXEL_LIMIT=256000000 # allow opening large sources while still downscaling
SOLAR_COCO_MAX_FRAMES=80               # cap frames processed per job (clamped internally)
SOLAR_COCO_SHARP_CONCURRENCY=1         # keep libvips concurrency low for stability
SOLAR_COCO_LOG_GEOM=0                  # set to 1 to log frame sizes
SOLAR_COCO_JOB_BUDGET_MS=0             # disable job budget checks by default; set >0 to enforce a limit
HEK_BASE_URL=http://www.lmsal.com/hek/her
HEK_EVENT_TYPES=ar,fl,ch,ef,cj         # default event types to request
HEK_RESULT_LIMIT=500
HEK_HPC_RADIUS=1200                    # arcsec half-width used for both query bounds and u,v mapping
HEK_TIMEOUT_MS=15000                   # ms timeout for HEK API calls
HEK_USE_SUNPY=0                        # set to 1 to invoke sunpy_bridge for precise HPC->(u,v)
SUNPY_PYTHON_BIN=python                # python executable to run tools/sunpy_bridge.py

# === Feature gates ===
ENABLE_REFLECTION=1
ENABLE_LOG_TEXT=1
ENABLE_EVAL_UI=0
ENABLE_TRACE_EXPORT=1
ENABLE_POLICY_REASONS=1
ENABLE_EVAL_REPLAY=0
ENABLE_ESSENCE_PROPOSALS=1
ENABLE_PROPOSAL_JOB_RUNNER=1
ENABLE_REPO_TOOLS=1

# QI autothrottle (enable to tame raw zeta)
QI_AUTOTHROTTLE_ENABLE=true
QI_AUTOTHROTTLE_TARGET=0.90
QI_AUTOTHROTTLE_HYST=0.05
QI_AUTOTHROTTLE_MIN=0.02
QI_AUTOTHROTTLE_ALPHA=0.25
QI_AUTOTHROTTLE_COOLDOWN_MS=1000

# QI autoscale (tile telemetry source)
QI_AUTOSCALE_ENABLE=true
QI_AUTOSCALE_TARGET=0.90       # keep zeta_raw below 0.90
QI_AUTOSCALE_SLEW=0.35         # faster pull-down for raw zeta
QI_AUTOSCALE_MIN_SCALE=0.03    # never drop below 3% drive
QI_AUTOSCALE_WINDOW_TOL=0.05   # require |sum(g*dt) - 1| <= 0.05
QI_AUTOSCALE_SOURCE=tile-telemetry

# TS autoscale (timescale guard; keeps tau_pulse <= tau_LC/target)
TS_AUTOSCALE_ENABLE=1
TS_AUTOSCALE_TARGET=120        # cushion above 100 to avoid rounding flips
TS_AUTOSCALE_SLEW=0.25         # 25%/s slew cap
TS_AUTOSCALE_FLOOR_NS=20       # never stretch pulses below 20 ns
TS_AUTOSCALE_WINDOW_TOL=0.05   # timing sanity band

# === Eval ===
EVAL_TASKS_FILE=./tests/evals/tasks.smoke.json
EVAL_SUCCESS_TARGET=0.70
EVAL_BASE_URL=http://localhost:3000

# === Holdout gate defaults (deployment) ===
AGI_HOLDOUT_MIN_PRECISION=0.95
AGI_HOLDOUT_MIN_RECALL=0.6
AGI_HOLDOUT_MAX_LATENCY_AVG_MS=45000
AGI_HOLDOUT_MAX_LATENCY_P95_MS=90000
AGI_HOLDOUT_MAX_LATENCY_P99_MS=120000

# === Specialists ===
ENABLE_SPECIALISTS=1
SPECIALISTS_MAX_REPAIR=1
SPECIALISTS_MAX_LATENCY_MS=120000
SPECIALISTS_DEFAULT_PERSONA=persona:demo
SOLVER_ALLOWLIST=
VERIFIER_ALLOWLIST=

ENABLE_PY_CHECKERS=0
PYTHON_BIN=python3
ENABLE_TRACE_API=1
TRACE_SSE_BUFFER=200
TRACE_TTL_MS=21600000

# === Tool log ingestion limits ===
TOOL_LOG_INGEST_MAX_BYTES=100000
TOOL_LOG_INGEST_MAX_RECORDS=200
TOOL_LOG_INGEST_RPM=0
TOOL_LOG_INGEST_RATE_WINDOW_MS=60000

# === Tool log audit log ===
TOOL_LOG_BUFFER_SIZE=200
TOOL_LOG_TENANT_BUFFER_SIZE=200
TOOL_LOG_PERSIST=0        # set to 1 to enable on-disk JSONL audit log
TOOL_LOG_AUDIT_DIR=.cal   # default when TOOL_LOG_AUDIT_PATH unset
# TOOL_LOG_AUDIT_PATH=./.cal/tool-log.jsonl
TOOL_LOG_ROTATE_MAX_BYTES=10000000
TOOL_LOG_ROTATE_MAX_FILES=5

# === Training trace audit log ===
TRAINING_TRACE_BUFFER_SIZE=200
TRAINING_TRACE_PERSIST=1        # set to 0 to disable on-disk JSONL audit log
TRAINING_TRACE_AUDIT_DIR=.cal   # default when TRAINING_TRACE_AUDIT_PATH unset
# TRAINING_TRACE_AUDIT_PATH=./.cal/training-trace.jsonl
TRAINING_TRACE_ROTATE_MAX_BYTES=20000000
TRAINING_TRACE_ROTATE_MAX_FILES=5

# === GR agent loop audit log ===
GR_AGENT_LOOP_BUFFER_SIZE=200
GR_AGENT_LOOP_PERSIST=1        # set to 0 to disable on-disk JSONL audit log    
GR_AGENT_LOOP_AUDIT_DIR=.cal   # default when GR_AGENT_LOOP_AUDIT_PATH unset    
# GR_AGENT_LOOP_AUDIT_PATH=./.cal/gr-agent-loop-audit.jsonl

# === Constraint pack policy profiles ===
CONSTRAINT_PACK_POLICY_BUFFER_SIZE=200
CONSTRAINT_PACK_POLICY_PERSIST=1        # set to 0 to disable on-disk JSONL log
CONSTRAINT_PACK_POLICY_AUDIT_DIR=.cal   # default when CONSTRAINT_PACK_POLICY_AUDIT_PATH unset
# CONSTRAINT_PACK_POLICY_AUDIT_PATH=./.cal/constraint-pack-policies.jsonl

# === Constraint pack telemetry ===
CASIMIR_AUTO_TELEMETRY=0
# CASIMIR_AUTO_CI_REPORTS=1
# CASIMIR_AUTO_CI_REPORTS_DIRS=reports
# CASIMIR_AUTO_CI_REPORTS_DIR=reports
# CASIMIR_REPORT_MAX_BYTES=25000000
# CASIMIR_TELEMETRY_PATH=./reports/telemetry.json
  # CASIMIR_REPO_TELEMETRY_PATH=./reports/repo-telemetry.json
  # CASIMIR_TOOL_TELEMETRY_PATH=./reports/tool-telemetry.json
  # CASIMIR_AUDIT_TELEMETRY_PATH=./reports/audit-telemetry.json
  # CASIMIR_TEST_JUNIT_PATH=./reports/junit.xml
  # JUNIT_PATH=
  # CASIMIR_TEST_VITEST_PATH=./reports/vitest.json
  # CASIMIR_TEST_JEST_PATH=./reports/jest.json
  # CASIMIR_LINT_ESLINT_PATH=./reports/eslint.json
  # CASIMIR_TYPECHECK_TSC_PATH=./reports/tsc.txt
  # CASIMIR_TOOL_LOG_TRACE_ID=
  # CASIMIR_TOOL_LOG_WINDOW_MS=600000
  # CASIMIR_TOOL_LOG_LIMIT=200
  CONSTRAINT_PACK_TELEMETRY_MAX_BYTES=5000000

# Repo convergence telemetry envs
# CASIMIR_BUILD_STATUS=pass
# CASIMIR_BUILD_OK=1
# CASIMIR_BUILD_EXIT_CODE=0
# CASIMIR_BUILD_DURATION_MS=420000
# CASIMIR_TEST_STATUS=pass
# CASIMIR_TEST_OK=1
# CASIMIR_TEST_FAILED=0
# CASIMIR_TEST_PASSED=128
# CASIMIR_TEST_TOTAL=128
# CASIMIR_SCHEMA_CONTRACTS=1
# CASIMIR_SCHEMA_OK=1
# CASIMIR_DEPS_COHERENCE=1
# CASIMIR_LINT_STATUS=1
# CASIMIR_TYPECHECK_STATUS=1
# CASIMIR_TIME_TO_GREEN_MS=480000
# CASIMIR_REPO_METRICS_JSON=

# Tool-use budget telemetry envs
# CASIMIR_STEPS_USED=12
# CASIMIR_STEPS_TOTAL=32
# CASIMIR_COST_USD=1.6
# CASIMIR_OPS_FORBIDDEN=0
# CASIMIR_OPS_APPROVAL_MISSING=0
# CASIMIR_PROVENANCE_MISSING=0
# CASIMIR_RUNTIME_MS=42000
# CASIMIR_TOOL_CALLS=6
# CASIMIR_TOOL_TOTAL=6
# CASIMIR_TOOL_METRICS_JSON=

# === OpenTelemetry tracing ===
OTEL_TRACING=0                 # set to 1 to enable span capture
OTEL_HTTP_SPANS=1              # set to 0 to skip HTTP request spans
OTEL_SERVICE_NAME=casimir-verifier
OTEL_SERVICE_VERSION=
OTEL_SERVICE_INSTANCE_ID=
OTEL_SPAN_PERSIST=0            # set to 1 to persist spans to JSONL
OTEL_SPAN_AUDIT_DIR=.cal       # default when OTEL_SPAN_AUDIT_PATH unset
# OTEL_SPAN_AUDIT_PATH=./.cal/otel-span.jsonl
OTEL_SPAN_BUFFER_SIZE=200
# OTEL_EXPORTER_OTLP_ENDPOINT=

# Debate mode
ENABLE_DEBATE=1
DEBATE_MAX_ROUNDS=6
DEBATE_MAX_WALL_MS=1200000
DEBATE_VERIFIERS=math.sympy.verify,code.verify.tests
ENABLE_DEBATE_SEARCH=1
DEBATE_SEARCH_DEFAULT=1    # if 1 and UI gate is on, select the Debate chip by default

# Debate UI (client)
VITE_ENABLE_DEBATE_UI=1
VITE_DEBATE_SSE_URL=/api/agi/tools/logs/stream
VITE_TRACE_API_BASE=/api/agi/trace
VITE_ENABLE_DEBATE_SEARCH=1
VITE_DEBATE_SEARCH_DEFAULT=1

# Local binary (no HTTP) LLM adapter
ENABLE_LLM_LOCAL_SPAWN=0
LLM_LOCAL_CMD=./.cache/llm/llama-prebuilt/llama-cli.exe
LLM_LOCAL_ARGS_BASE=
LLM_LOCAL_CPU_BACKEND=sse42  # win32: pin ggml-cpu backend (sse42|avx|avx2|haswell)
LLM_LOCAL_MODEL=./models/Local-7B-Q4.gguf  # file path
LLM_LOCAL_LORA_PATH=
LLM_LOCAL_CONTEXT_TOKENS=2048
LLM_LOCAL_MAX_TOKENS=2048
LLM_LOCAL_TEMP=0.2
LLM_LOCAL_SEED=42
LLM_LOCAL_SPAWN_TIMEOUT_MS=300000


# --- OpenAI wiring profile (example) ---
# Core posture
# ENABLE_ESSENCE=1
# ENABLE_AGI=1
# ENABLE_LOG_TEXT=1
# HULL_MODE=1
# HULL_ALLOW_HOSTS=api.openai.com

# Text LLM (chat/completions)
# LLM_HTTP_BASE=https://api.openai.com
# LLM_HTTP_API_KEY=<YOUR_OPENAI_KEY>
# LLM_HTTP_MODEL=gpt-4o-mini

# Vision (image â†’ text) also uses chat/completions
# VISION_HTTP_BASE=https://api.openai.com
# VISION_HTTP_API_KEY=<YOUR_OPENAI_KEY>
# VISION_HTTP_MODEL=gpt-4o-mini

# Speech-to-Text (Whisper)
# WHISPER_HTTP_URL=https://api.openai.com
# WHISPER_HTTP_API_KEY=<YOUR_OPENAI_KEY>
# WHISPER_HTTP_MODE=openai

# Optional console niceties
# ENABLE_TRACE_EXPORT=1
# ENABLE_POLICY_REASONS=1
# ENABLE_EVAL_REPLAY=1

REMOVE_BG_PYTHON_BIN=C:\Python313\python.exe
