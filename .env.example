PORT=5173
# If your API runs on a separate port, set API_PROXY_TARGET to that URL (e.g., http://localhost:3000).
# When Express + Vite share the same port, leave this unset to avoid proxy loops.
# API_PROXY_TARGET=http://localhost:3000

ENABLE_ESSENCE=1      # set to 0 to disable Essence APIs
ENABLE_AGI=1          # set to 0 to disable AGI planner routes
ENABLE_AGI_UI=1
ENABLE_PERSONA_UI=1
ENABLE_MEMORY_UI=1
ENABLE_PHYSICS=0
HELIX_MODEL_MODE=raw  # raw = Mk.1/paper profile (requested gap); calibrated = enforce mechanical guard
PHYSICS_MODEL=gpt-4o-mini
# Warp explainer backend (physics.warp.ask): openai | ollama | auto
BACKEND_PHYSICS_ASK=openai
PHYSICS_ASK_OPENAI_MODEL=gpt-4.1-mini
PHYSICS_ASK_LOCAL_MODEL=llama3.1-70b-instruct
ENABLE_LATTICE_WATCHER=0
LATTICE_WATCHER_DEBOUNCE_MS=350
VITE_CHAT_CONTEXT_BUDGET_TOKENS=8192
VITE_HELIX_DEV_MOCKS=0     # set to 0 to surface real Helix API errors instead of mocks
VITE_DEBUG_VIZ_HUD=1       # set to 1 to show Viz Diagnostics HUD on Helix Start
VITE_ENABLE_LATTICE_WATCHER=0
ENABLE_RESONANCE_ORCHESTRA=1
VITE_ENABLE_RESONANCE_ORCHESTRA=1
HYBRID_COLLAPSE_MODE=deterministic_hash_v1   # deterministic_hash_v1 | micro_llm_v1 | embedding_v1 | off
VITE_HYBRID_COLLAPSE_MODE=deterministic_hash_v1
VITE_LOCAL_CHOOSER_URL=                      # optional local micro-LLM chooser endpoint
FORCE_FULL_PLAN=1                           # set to 1 to disable direct answers and force full planning path
VITE_FORCE_FULL_PLAN=1
ENABLE_LOCAL_CALL_SPEC=0                    # set to 1 to enable local call spec fetch (edge micro-LLM)
VITE_ENABLE_LOCAL_CALL_SPEC=0
VITE_LOCAL_CALL_SPEC_URL=http://localhost:11434/api/local-call-spec

# LLM runtime selection and paths (user will set real paths locally)
LLM_RUNTIME=llama.cpp
LLM_MODEL_PATH=/models/8b-q4.gguf
LLM_DRAFT_MODEL_PATH=/models/3b-q4.gguf
LLM_USE_SPECULATIVE=true

# KV budget (~1.5 GB by default for laptop VRAM)
KV_BUDGET_BYTES=1500000000
KV_EVICT_STRATEGY=oldest

# RAG / Memory
EMBEDDING_SPACE=text-bert-base
MEM_TOPK=6

# Diffusion defaults (adapter will exist later)
DIFF_ENGINE=sd15-lcm
DIFF_STEPS=4
DIFF_SLICING=true

# Persistence / storage
USE_INMEM_MEMORY=1
ENABLE_PGVECTOR=0
STORAGE_BACKEND=fs
DATA_DIR=./data/essence
REDIS_URL=
MEDIA_QUEUE_CONCURRENCY=1
MEDIA_GPU_CONCURRENCY=1
MEDIA_QUEUE_BYPASS=0
S3_BUCKET=essence-artifacts
S3_REGION=us-east-1
S3_ENDPOINT=
S3_FORCE_PATH_STYLE=1
S3_ACCESS_KEY_ID=
S3_SECRET_ACCESS_KEY=

# Auth
ENABLE_AUTH=0
ALLOW_ADMIN=0
JWT_SECRET=super-secret

# GPU safety
GPU_TEMP_MAX_C=82
GPU_TEMP_OVERRIDE=

# === Knowledge Projects / Attachments ===
ENABLE_KNOWLEDGE_PROJECTS=1
MAX_KNOWLEDGE_CONTEXT_BYTES=262144         # 256 KB total payload per plan
MAX_KNOWLEDGE_FILES_PER_PROJECT=64
KNOWLEDGE_ALLOWED_MIME=text/plain,text/markdown,application/json,application/pdf,audio/wav,audio/mpeg
# Planner hinting toggles (optional)
ENABLE_KNOWLEDGE_PROMPT_HINTS=1

# === Hull Mode (offline) ===
HULL_MODE=0
# Add external hosts you trust in Hull Mode (comma-separated). Example to allow OpenAI:
# HULL_ALLOW_HOSTS=api.openai.com,127.0.0.1,::1,localhost,*.hull
HULL_ALLOW_HOSTS=127.0.0.1,::1,localhost,*.hull
LLM_POLICY=local
LLM_LOCAL_BASE=http://127.0.0.1:11434
LLM_LOCAL_MODEL=local-7b-instruct
ENABLE_CAPSULE_IMPORT=1
HULL_CA_PUBKEY_PATH=./ops/hull-ca.pub

# === LLM (OpenAI-compatible HTTP, e.g., llama.cpp server, vLLM, Ollama / OpenAI) ===
LLM_HTTP_BASE=
LLM_HTTP_API_KEY=
LLM_HTTP_MODEL=gpt-4o-mini
LLM_HTTP_TEMPERATURE=0.2

# === Luma summarizer (OpenAI/ollama/vLLM) ===
LUMA_PROVIDER=ollama           # openai | ollama | vllm
LUMA_MODEL=mistral:7b-instruct # override the provider’s default model
LUMA_BASE_URL=http://127.0.0.1:5173
OLLAMA_ENDPOINT=http://127.0.0.1:11434
LORA_ADAPTER=
OLLAMA_PROFILE_MODEL=mistral:7b-instruct

# === Vision (OpenAI-compatible Chat Completions with images) ===
VISION_HTTP_BASE=
VISION_HTTP_API_KEY=
VISION_HTTP_MODEL=gpt-4o-mini
VISION_HTTP_RPM=60

# === Local TTS (custom backend, e.g., AudioCraft wrapper) ===
ENABLE_LOCAL_TTS=0                       # set to 1 to enable /tts/local proxy
VITE_ENABLE_LOCAL_TTS=0                  # client flag if needed
LOCAL_TTS_URL=http://127.0.0.1:8000/api/tts

# === STT (OpenAI /audio/transcriptions or generic) ===
WHISPER_HTTP_MODE=openai     # openai | generic
WHISPER_HTTP_URL=
WHISPER_HTTP_API_KEY=

# === Diffusion (AUTOMATIC1111 SD WebUI or ComfyUI) ===
DIFF_HTTP_ENGINE=sdwebui     # sdwebui | comfyui
DIFF_HTTP_URL=
DIFF_STEPS=6
DIFF_WIDTH=512
DIFF_HEIGHT=512
DIFF_SAMPLER=LCM
DIFF_SEED=-1                 # -1 = backend random

# === Solar coherence (Star Watcher) ===
SOLAR_COCO_MAX_PIXELS=4000000          # per-frame target pixels (downscale to this budget)
SOLAR_COCO_INPUT_PIXEL_LIMIT=256000000 # allow opening large sources while still downscaling
SOLAR_COCO_MAX_FRAMES=80               # cap frames processed per job (clamped internally)
SOLAR_COCO_SHARP_CONCURRENCY=1         # keep libvips concurrency low for stability
SOLAR_COCO_LOG_GEOM=0                  # set to 1 to log frame sizes
SOLAR_COCO_JOB_BUDGET_MS=0             # disable job budget checks by default; set >0 to enforce a limit
HEK_BASE_URL=http://www.lmsal.com/hek/her
HEK_EVENT_TYPES=ar,fl,ch,ef,cj         # default event types to request
HEK_RESULT_LIMIT=500
HEK_HPC_RADIUS=1200                    # arcsec half-width used for both query bounds and u,v mapping
HEK_TIMEOUT_MS=15000                   # ms timeout for HEK API calls
HEK_USE_SUNPY=0                        # set to 1 to invoke sunpy_bridge for precise HPC->(u,v)
SUNPY_PYTHON_BIN=python                # python executable to run tools/sunpy_bridge.py

# === Feature gates ===
ENABLE_REFLECTION=1
ENABLE_LOG_TEXT=1
ENABLE_EVAL_UI=0
ENABLE_TRACE_EXPORT=0
ENABLE_POLICY_REASONS=1
ENABLE_EVAL_REPLAY=0
ENABLE_ESSENCE_PROPOSALS=1
ENABLE_PROPOSAL_JOB_RUNNER=1

# QI autothrottle (opt-in; defaults off)
QI_AUTOTHROTTLE_ENABLE=false
QI_AUTOTHROTTLE_TARGET=0.90
QI_AUTOTHROTTLE_HYST=0.05
QI_AUTOTHROTTLE_MIN=0.02
QI_AUTOTHROTTLE_ALPHA=0.25
QI_AUTOTHROTTLE_COOLDOWN_MS=1000

# QI autoscale (tile telemetry source) - defaults ON for safe-by-default native bundle
QI_AUTOSCALE_ENABLE=true
QI_AUTOSCALE_TARGET=0.90       # keep zeta_raw below 0.90
QI_AUTOSCALE_SLEW=0.35         # 35%/s slew cap (follows guard recompute)
QI_AUTOSCALE_MIN_SCALE=0.03    # never drop below 3% drive
QI_AUTOSCALE_WINDOW_TOL=0.05   # require |sum(g*dt) - 1| <= 0.05
QI_AUTOSCALE_SOURCE="tile-telemetry"  # only act on live tile rho

# Averaging/TS autoscale (default on for native bundle)
TS_AUTOSCALE_ENABLE=1
TS_AUTOSCALE_TARGET=120        # cushion above the 100 soft floor to avoid rounding flips
TS_AUTOSCALE_SLEW=0.25         # 25%/s slew cap
TS_AUTOSCALE_FLOOR_NS=20       # never stretch pulses below 20 ns
TS_AUTOSCALE_WINDOW_TOL=0.05   # timing sanity band

# === Eval ===
EVAL_TASKS_FILE=./tests/evals/tasks.smoke.json
EVAL_SUCCESS_TARGET=0.70
EVAL_BASE_URL=http://localhost:3000

# === Specialists ===
ENABLE_SPECIALISTS=1
SPECIALISTS_MAX_REPAIR=1
SPECIALISTS_MAX_LATENCY_MS=120000
SPECIALISTS_DEFAULT_PERSONA=persona:demo
SOLVER_ALLOWLIST=
VERIFIER_ALLOWLIST=

ENABLE_PY_CHECKERS=0
PYTHON_BIN=python3
ENABLE_TRACE_API=1
TRACE_SSE_BUFFER=200
TRACE_TTL_MS=21600000

# Debate mode
ENABLE_DEBATE=1
DEBATE_MAX_ROUNDS=6
DEBATE_MAX_WALL_MS=1200000
DEBATE_VERIFIERS=math.sympy.verify,code.verify.tests
ENABLE_DEBATE_SEARCH=1
DEBATE_SEARCH_DEFAULT=1    # if 1 and UI gate is on, select the Debate chip by default

# Debate UI (client)
VITE_ENABLE_DEBATE_UI=1
VITE_DEBATE_SSE_URL=/api/agi/tools/logs/stream
VITE_TRACE_API_BASE=/api/agi/trace
VITE_ENABLE_DEBATE_SEARCH=1
VITE_DEBATE_SEARCH_DEFAULT=1

# Local binary (no HTTP) LLM adapter
ENABLE_LLM_LOCAL_SPAWN=1
LLM_LOCAL_CMD=./bin/llama-cli              # your local binary path
LLM_LOCAL_ARGS_BASE=--n-gpu-layers=20 --ctx-size=4096
LLM_LOCAL_MODEL=./models/Local-7B-Q4.gguf  # file path
LLM_LOCAL_MAX_TOKENS=512
LLM_LOCAL_TEMP=0.2
LLM_LOCAL_SEED=42


# --- OpenAI wiring profile (example) ---
# Core posture
# ENABLE_ESSENCE=1
# ENABLE_AGI=1
# ENABLE_LOG_TEXT=1
# HULL_MODE=1
# HULL_ALLOW_HOSTS=api.openai.com

# Text LLM (chat/completions)
# LLM_HTTP_BASE=https://api.openai.com
# LLM_HTTP_API_KEY=<YOUR_OPENAI_KEY>
# LLM_HTTP_MODEL=gpt-4o-mini

# Vision (image → text) also uses chat/completions
# VISION_HTTP_BASE=https://api.openai.com
# VISION_HTTP_API_KEY=<YOUR_OPENAI_KEY>
# VISION_HTTP_MODEL=gpt-4o-mini

# Speech-to-Text (Whisper)
# WHISPER_HTTP_URL=https://api.openai.com
# WHISPER_HTTP_API_KEY=<YOUR_OPENAI_KEY>
# WHISPER_HTTP_MODE=openai

# Fashion generator
# IMAGE_LOOKS_MODEL=gpt-image-1
# FASHION_TEMPLATE_DIR=./templates/fashion

# Optional console niceties
# ENABLE_TRACE_EXPORT=1
# ENABLE_POLICY_REASONS=1
# ENABLE_EVAL_REPLAY=1

